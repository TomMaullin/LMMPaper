{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Estimation Simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides an overview of how the parameter estimation simulations presented in the paper \"A Fisher Scoring approach for crossed multiple-factor Linear Mixed Models\" were performed using the code available in this repository. \n",
    "\n",
    "As the simulations in the paper were run on a SGE cluster set-up (and the scripts used were specific to the cluster and software employed), we provide details on how to run individual simulations and how to combine the results from simulations which have already been run but do not provide scripts for submitting jobs to a cluster. This notebook should make it clear how the simulations can be run on a cluster but, to do so, you will need to write your own cluster-specific scripts for the actual job submission. The file `src/Simulations/LMMPaperSim.sh` provides some initial bash scripts which may help you to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import numpy as np\n",
    "import scipy\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import sys\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Import modules from elsewhere in the repository.\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(os.path.join(module_path,\"src\",\"Simulations\"))\n",
    "    sys.path.append(os.path.join(module_path,\"src\",\"lib\"))\n",
    "    \n",
    "from genTestDat import genTestData2D, prodMats2D\n",
    "from LMMPaperSim import *\n",
    "from est2d import *\n",
    "from npMatrix2d import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `runSim`, which can be found in `LMMPaperSim.py`, can be used to run a single simulation instance. It takes the following inputs:\n",
    "\n",
    " - `simInd`: An index to represent the simulation. All output for this simulation will be saved in files with the index specified by this argument. The simulation with index 1 will also perform any necessary additional setup and should therefore be run before any others.\n",
    " - `desInd`: Integer value between 1 and 3 representing which design to run. The designs are as follows:\n",
    "   - `Design 1: nlevels=[50], nraneffs=[2]`\n",
    "   - `Design 2: nlevels=[50,10], nraneffs=[3,2]`\n",
    "   - `Design 3: nlevels=[100,50,10], nraneffs=[4,3,2]`\n",
    " - `OutDir`: The output directory.\n",
    " - `mode`: String indicating whether to run parameter estimation simulations (`mode='param'`) or T statistic simulations (`mode='Tstat'`).\n",
    " - `REML`: Boolean indicating whether to use ML or ReML estimation. \n",
    "\n",
    "An example of how this function can be used is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a simulation with index 1\n",
    "simInd = 1\n",
    "\n",
    "# Use design 2\n",
    "desInd = 2\n",
    "\n",
    "# Set output directory\n",
    "OutDir = os.path.join(module_path,\"data\",\"ParamSimulation\")\n",
    "\n",
    "# Make the output directory if it doesn't exist already.\n",
    "if not os.path.isdir(OutDir):\n",
    "    os.mkdir(OutDir)\n",
    "\n",
    "# Run a simulation\n",
    "runSim(simInd, desInd, OutDir, mode='param', REML=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example output from running one simulation is given below. The files output for the $i^{th}$ simulation running the $j^{th}$ design are as follows:\n",
    "\n",
    "- `Sim{i}_Design{j}_X.csv`: The fixed effects design used for the simulation.\n",
    "- `Sim{i}_Design{j}_Y.csv`: The response vector used for the simulation.\n",
    "- `Sim{i}_Design{j}_Zfactor{k}.csv`: The factor vector for the $k^{th}$ random factor (See Appendix $6.1$ of the main paper for more details).\n",
    "- `Sim{i}_Design{j}_Zdata{k}.csv`: The raw regressor matrix for the $k^{th}$ random factor (See Appendix $6.1$ of the main paper for more details).\n",
    "- `Sim{i}_Design{j}_results.csv`: The results of the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List contents of output directory.\n",
    "os.listdir(OutDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below provides a display of what can be found in the 'results' file listed above. The first column provides the ground truth used for this simulation and the other columns provide the results of each Fisher Scoring method. The column indices are: \n",
    "\n",
    " - `Time`: Time (in seconds) taken for computation.\n",
    " - `nit`: Number of iterations performed.\n",
    " - `llh`: Maximised (restricted) Log-likelihood function.\n",
    " - `beta{i}`: The $i^{th}$ fixed effects estimate, $\\beta$.\n",
    " - `sigma2`: The fixed effects variance, $\\sigma^2$.\n",
    " - `D{k},{j}`: The $j^{th}$ estimated component of vech$(D_k)$.\n",
    " - `sigma2*D{k},{j}`: The fixed effects variance, $\\sigma^2$, multiplied by the $j^{th}$ estimated component of vech$(D_k)$ (often any further computation is more concerned with this quantity than the seperate $\\sigma^2$ and $D$ components). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the results file.\n",
    "results_table = pd.read_csv(os.path.join(OutDir,'Sim1_Design2_results.csv'),index_col=0)\n",
    "\n",
    "# Display results\n",
    "results_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results may appear alarming on first view as it appears that the parameter estimates differ from the ground truth notably. However, comparing the log-likelihood values in the third row, it can be seen that the ground truth parameters do not correspond exactly to the maxima of the log-likelihood function. Therefore, it can be seen that the observed differences are a feature inherent to likelihood maximisation in general, rather than a byproduct of the Fisher Scoring method. In other words, as expected, the likeliest estimate for a parameter given some specific data is not exactly equal to the true underlying expected value of the parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running multiple simulations in serial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many simulations can be run in serial. The function `sim2D` can be used to do this. It takes the following inputs:\n",
    "\n",
    " - `desInd`: Integer value between 1 and 3 representing which design to run. The designs are as follows:\n",
    "   - `Design 1: nlevels=[50], nraneffs=[2]`\n",
    "   - `Design 2: nlevels=[50,10], nraneffs=[3,2]`\n",
    "   - `Design 3: nlevels=[100,50,10], nraneffs=[4,3,2]`\n",
    " - `OutDir`: The output directory.\n",
    " - `nsim`: Number of simulations (default=`1000`)\n",
    " - `mode`: String indicating whether to run parameter estimation simulations (`mode='param'`) or T statistic simulations (`mode='Tstat'`).\n",
    " - `REML`: Boolean indicating whether to use ML or ReML estimation. \n",
    "\n",
    "An example of how this function can be used is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 3 simulation instances\n",
    "sim2D(desInd, OutDir, nsim=3, mode='param', REML=False)\n",
    "\n",
    "# List contents of output directory.\n",
    "os.listdir(OutDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running multiple simulations in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned at the start of this document, the simulation functions for the Fisher Scoring methods have been designed with cluster computation in mind. The file `LMMPaperSim.sh` contains bash commands and commented suggestions for how to run these functions on a cluster. As cluster set-ups vary notably from lab to lab we cannot provide exact commands for cluster submission but hope that this notebook and the surrounding files should be comprehensive enough so that anyone who wishes to submit the code to a cluster can do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running `lmer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, we now run the same simulations in `lmer`. This is why we previously saved the `X`, `Y` and `Z` files for each simulation. To do this the `LMMPaperSim.r` code must be run in the programming language `R`. Before running this code, we suggest you first open the file to check that the input options (i.e. the number of simulations the code thinks we have run, the design index,... etc.) match those used above. Once you have done this, you can either run the file manually or source it in the `R` command line using the below command (with the path changed appropriately)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`source('~/Path/To/Repository/LMMPaper/src/Simulations/LMMPaperSim.r')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this has been done, you will find a column has been added to the results file for `lmer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the results file.\n",
    "results_table = pd.read_csv(os.path.join(OutDir,'Sim1_Design2_results.csv'),index_col=0)\n",
    "\n",
    "# Display results\n",
    "results_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The `R` code above must be run before moving onto the following sections of this notebook. This is as the following python code is expecting results from `lmer` to now be recorded in the results file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the simulations have been run, the results may be combined across simulations using the following functions;\n",
    "\n",
    " - `differenceMetrics`: This generates MAE and MRD values.\n",
    " - `performanceTables`: This generates the performance metrics.\n",
    " \n",
    "These functions were used to produce the results displayed in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `performanceTables` calculates the performance metrics (i.e. the computation time, number of iterations and maximized log-likehood criteria) for each simulation and stores the results in tables. The [pandas description](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html) of the tables are printed as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performanceTables(desInd, OutDir, nsim=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code can be used to see which files have now been created. `llhTable.csv` contains the maximized likelihood values for each method in each simulation. `nitTable.csv` contains the number of iterations used for each method in each simulation. `timesTable.csv` contains the times taken for each method in each simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List contents of output directory.\n",
    "glob.glob(os.path.join(OutDir,'*Table.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below one of the files, the table of performance times, is displayed. The row index corresponds to simulation number and the column index corresponds to the method used for parameter estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the results file.\n",
    "times_table = pd.read_csv(os.path.join(OutDir,'timesTable.csv'),index_col=0)\n",
    "\n",
    "# Display results\n",
    "times_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `differenceMetrics` calculates the MAE (Mean Absolute Error) and MRD (Mean Relative Difference) metrics for each simulation and stores the results in tables. The [pandas description](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html) of the tables are printed as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "differenceMetrics(desInd, OutDir, nsim=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code can be used to see which files have now been created. The suffix `_abs` refers to MAE (mean ABSolute error) and the suffix `_rel` refers to MRD (mean RELative differences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List contents of output directory.\n",
    "glob.glob(os.path.join(OutDir,'diff*'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below one of the files, the MAE values taken with respect to the ground truth used for simulation, for the beta estimates, is displayed. The row index corresponds to simulation number and the column index corresponds to the method used for parameter estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the results file.\n",
    "MAE_table = pd.read_csv(os.path.join(OutDir,'diffTableBetas_truth_abs.csv'),index_col=0)\n",
    "\n",
    "# Display results\n",
    "MAE_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
